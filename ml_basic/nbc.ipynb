{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "389991c7",
   "metadata": {},
   "source": [
    "## Bag of Word (Túi đựng từ) - Văn bản được mã hóa như thế nào?\n",
    "\n",
    "Chúng ta có tập các văn bản $\\mathbf{X}$, chúng ta sẽ liệt kê ra tất cả các từ có trong $\\mathbf{X}$ và đếm được có $d$ từ, đây là độ dài của vector mã hóa. Sau đó, với mỗi văn bản $\\mathbf{x}$ chúng ta liệt kê ra tất cả các từ có trong, đếm số lượng và mã hóa chúng thành một vector $x$.\n",
    "$$\n",
    "\\mathbf{x} = [x_1, x_2, ..., x_d]\n",
    "$$\n",
    "#### Ví dụ:\n",
    "Chúng ta có các văn bản sau:\n",
    "$$\n",
    "\\mathbf{d1} = \\text{\"con mèo uia\"} \\\\\n",
    "\\mathbf{d2} = \\text{\"con chó mực là bạn con mèo uia\"} \n",
    "$$\n",
    "Tập các văn bản này là:\n",
    "$$\n",
    "\\mathbf{X} = \\set{\\mathbf{d1}, \\mathbf{d2}}\n",
    "$$\n",
    "\n",
    "Danh sách các từ có trong $\\mathbf{X}$:\n",
    "$$\n",
    "\\mathbf{v} = \\set{\\text{\"con\"}, \\text{\"mèo\"}, \\text{\"uia\"}, \\text{\"chó\"}, \\text{\"mực\"}, \\text{\"là\"}, \\text{\"bạn\"}}\n",
    "$$\n",
    "Khi đó, các văn bản được mã hóa như sau:\n",
    "$$\n",
    "\\mathbf{d1} = [1, 1, 1, 0, 0, 0, 0] \\\\\n",
    "\\mathbf{d2} = [2, 1, 1, 1, 1, 1, 1]\n",
    "$$\n",
    "Chúng ta có thể thấy dù tập văn bản có kích thước khác nhau, nhưng khi mã hóa thì chúng sẽ có cùng độ dài vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "94b8e062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class BoW:\n",
    "    def tokenize(self, text):\n",
    "        return text.lower().split()\n",
    "\n",
    "    def build_vocab(self, docs):\n",
    "        vocab = set()\n",
    "        for doc in docs:\n",
    "            tokens = self.tokenize(doc)\n",
    "            vocab.update(tokens)\n",
    "        return sorted(vocab)\n",
    "\n",
    "    def text_to_bow(self, text, vocab):\n",
    "        tokens = self.tokenize(text)\n",
    "        bow = [0] * len(vocab)\n",
    "        for token in tokens:\n",
    "            if token in vocab:\n",
    "                idx = vocab.index(token)\n",
    "                bow[idx] += 1\n",
    "        return bow\n",
    "\n",
    "    def docs_to_bow_matrix(self, docs, vocab=None):\n",
    "        if vocab is None:\n",
    "            vocab = self.build_vocab(docs)\n",
    "        matrix = []\n",
    "        for doc in docs:\n",
    "            vector = self.text_to_bow(doc, vocab)\n",
    "            matrix.append(vector)\n",
    "        return np.array(matrix), vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "e88460e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['bạn', 'chó', 'con', 'là', 'mèo', 'mực', 'uia']\n",
      "BoW matrix:\n",
      " [[0 0 1 0 1 0 1]\n",
      " [1 1 2 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "docs = [\n",
    "    \"con mèo uia\",\n",
    "    \"con chó mực là bạn con mèo uia\",\n",
    "]\n",
    "\n",
    "X, vocab = BoW().docs_to_bow_matrix(docs)\n",
    "print(\"Vocabulary:\", vocab)\n",
    "print(\"BoW matrix:\\n\", X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c34f1c8",
   "metadata": {},
   "source": [
    "##  Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f82492",
   "metadata": {},
   "source": [
    "Trên một tập dữ liệu văn bản đã được đánh nhãn. Có một văn bản mới là $\\mathbf{x}$, ta cần quyết định xem điểm $\\mathbf{x}$ thuộc vào class $c$ nào?\n",
    "\n",
    "Một trong những cách là xét xác suất văn bản $\\mathbf{x}$ thuộc vào từng class là bao nhiêu, và sẽ chọn ra class có xác suất thuộc là lớn nhất!\n",
    "\n",
    "Tư duy một cách thông thường, ta sẽ quan sát các đặt trưng của văn bản $\\mathbf{x}$ và quyết định xem class nào phù hợp với đặc trưng đó, nhưng vậy các xác suất cần tính sẽ là:\n",
    "$$\n",
    "P(y=c|\\mathbf{x}) = P(c|\\mathbf{x})\n",
    "$$\n",
    "\n",
    "Để tính xác suất trên, ta quan sát bộ dữ liệu, chọn ra tập văn bản $\\mathbf{X}$ có đặc trưng $\\mathbf{x}$, tập này có lực lượng (số phần tử) là $|\\mathbf{X}|$, trong tập $\\mathbf{X}$ đếm được $N_{\\mathbf{x}c}$ phần tử có label là $c$. Khi đó: \n",
    "$$\n",
    "P(c|\\mathbf{x}) = \\frac{N_{\\mathbf{x}c}}{|\\mathbf{X}|}\n",
    "$$\n",
    "\n",
    "Một vấn đề thách thức đó chính là tập dữ liệu văn bản của chúng ta khi thu thập không đủ lớn, có khi văn bản đang cần đáng nhãn không xuất hiện trong dữ liệu chúng ta thu thâp được, vì vậy chúng ta đi tính gián tiếp thông qua công thức Bayes:\n",
    "$$\n",
    "P(c|\\mathbf{x}) = \\frac{P(\\mathbf{x}c)}{P(\\mathbf{x})} = \\frac{P(\\mathbf{x}|c)P(c)}{P(\\mathbf{x})} \n",
    "$$\n",
    "\n",
    "Trong các xác suất trên, ta sẽ chọn ra class $c$ có xác suất cao nhất\n",
    "$$\n",
    "c = \\argmax _{c \\in \\{1,...,k\\}}{P(c|\\mathbf{x})} \n",
    "= \\argmax _{c \\in \\{1,...,k\\}}{\\frac{P(\\mathbf{x}|c)P(c)}{P(\\mathbf{x})}} \n",
    "= \\argmax _{c \\in \\{1,...,k\\}}{P(\\mathbf{x}|c)P(c)} \\space (*)\n",
    "$$\n",
    "\n",
    "Giá trị $P(c)$ là xác suất văn bản có nhãn $c$ được xác định như sau:\n",
    "$$\n",
    "P(c) = \\frac{|\\mathbf{C}|}{N}\n",
    "$$\n",
    "Trong đó:\n",
    "- $\\mathbf{C}$ tập văn bản có nhãn $c$ và $|\\mathbf{C}|$ số dữ liệu thuộc vào class $c$  \n",
    "- $N$ số điểm dữ liệu\n",
    "\n",
    "Giá trị $P(\\mathbf{x}|c)$ là xác suất văn bản $\\mathbf{x}$ suất hiện trong tập $\\mathbf{C}$ được xác định như sau, với $\\mathbf{x}$ được mã hóa thành **BoW**:\n",
    "$$\n",
    "P(\\mathbf{x}|c) = P(x_1, ..., x_d | c) \\space (*)\n",
    "$$\n",
    "\n",
    "Và xác suất này cũng rất khó tính (do thực tế không thể thu thập đủ dữ liệu), giả sử rằng văn bản $\\mathbf{x}$ có $d$ từ $x_i$ khác nhau $(*)$, và cũng giả sử rằng các từ $x_i$ trong $\\mathbf{x}$ độc lập với nhau tức sự xác suất xuất hiện của từ này không phụ thuộc (cũng như ảnh hưởng) vào sự xuất hiện của các từ khác (thực tế là có), khi đó:\n",
    "$$\n",
    "P(\\mathbf{x}|c) = P(x_1, ..., x_d | c) = P(x_1|c)\\cdot P(x_2|c)\\cdot...\\cdot P(x_d|c) = \\prod_{i = 1}^d{P(x_i|c)}\n",
    "$$\n",
    "\n",
    "Xác suất $P(x_i|c)$ là xác suất từ $x_i$ xuất hiện trong tập dữ liệu có nhãn là $c$ được xác định như sau:\n",
    "$$\n",
    "P(x_i|c) = \\frac{N_{cx_i}}{N_c} \n",
    "$$\n",
    "Trong đó:\n",
    "- $N_{cx_i}$ là số lượng $x_i$ có nhãn $c$, khi các văn bản được vector hóa $N_{cx_i} = \\sum_{\\mathbf{x} \\in \\mathbf{C}}{x_i}$ \n",
    "- $N_c$ là tổng số lượng từ có trong các phần tử có nhãn $c$ trong tập dữ liệu\n",
    "\n",
    "Thực tế, giá trị $P(x_i|c)$ theo công thức trên có thể bằng 0, điều này sẽ dẫn đến không thể tìm được nghiệm của phương trình $(*)$, một kỹ thuật được áp dụng được gọi là ***Laplace Smoothing***, khi đó $P(x_i|c)$ được xác định:\n",
    "$$\n",
    "P(x_i|c) = \\frac{N_{cx_i} + \\alpha}{N_c + d\\alpha} \n",
    "\n",
    "$$\n",
    "\n",
    "Vậy phương trình cần phải giải:\n",
    "$$\n",
    "c \n",
    "= \\argmax_{c \\in {1, \\ldots, k}}{P(\\mathbf{x}|c)P(c)} \\\\\n",
    "= \\argmax_{c \\in {1, \\ldots, k}}{P(c)\\prod_{i = 1}^d{P(x_i|c)}} \\\\\n",
    "= \\argmax_{c \\in \\{1, \\ldots, k\\}} \\left[ \\ln P(c) + \\sum_{i = 1}^d \\ln P(x_i \\mid c) \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "1c2a2cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NBModel:\n",
    "    def __init__(self, alpha = 1):\n",
    "        self.alpha   = alpha\n",
    "        self.dataset = None\n",
    "        self.labels  = None\n",
    "        self.log_lambda  = []\n",
    "        self.log_prior   = []\n",
    "        self.unique_labels = []\n",
    "\n",
    "    def fit(self, X_train, Y_train):\n",
    "        self.dataset = X_train\n",
    "        self.labels  = Y_train\n",
    "        self.unique_labels = np.unique(Y_train)\n",
    "\n",
    "        for label in self.unique_labels:\n",
    "            subset = self.dataset[self.labels == label]\n",
    "            N_cx = np.sum(subset, axis=0)\n",
    "            N_c  = np.sum(N_cx)\n",
    "            prob = (N_cx + self.alpha) / (N_c + len(N_cx) * self.alpha)\n",
    "            self.log_lambda.append(np.log(prob))\n",
    "            self.log_prior.append(np.log(len(subset) / len(self.dataset)))\n",
    "    \n",
    "    def log_likelihood(self, x, idx):\n",
    "        return np.sum(x * self.log_lambda[idx])\n",
    "\n",
    "    def compute_log_posterior(self, x):\n",
    "        log_probs = []\n",
    "        for idx in range(len(self.unique_labels)):\n",
    "            log_p = self.log_prior[idx] + self.log_likelihood(x, idx)\n",
    "            log_probs.append(log_p)\n",
    "        return np.array(log_probs)\n",
    "    \n",
    "    def predict(self, X_pred):\n",
    "        labels = []\n",
    "        for x in X_pred:\n",
    "            log_probs = self.compute_log_posterior(x)\n",
    "            pred_label = self.unique_labels[np.argmax(log_probs)]\n",
    "            labels.append(pred_label)\n",
    "        return np.array(labels)\n",
    "\n",
    "    def predict_proba(self, X_pred):\n",
    "        probas = []\n",
    "        for x in X_pred:\n",
    "            log_probs = self.compute_log_posterior(x)\n",
    "            max_log = np.max(log_probs)\n",
    "            stable_log_probs = log_probs - max_log\n",
    "            probs = np.exp(stable_log_probs)\n",
    "            probs /= np.sum(probs)\n",
    "            probas.append(probs)\n",
    "        return np.array(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "d3b95f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['banhbo', 'banhgio', 'buncha', 'chaolong', 'hanoi', 'hutiu', 'omai', 'pho', 'saigon']\n",
      "Train_data:\n",
      "[[0 0 0 1 2 0 0 1 0]\n",
      " [0 0 1 0 1 0 1 1 0]\n",
      " [0 1 0 0 0 0 1 1 0]\n",
      " [1 0 0 0 0 1 0 1 1]]\n",
      "\n",
      "Test_data:\n",
      "[[0 0 1 0 2 1 0 0 0]\n",
      " [1 0 0 0 0 1 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# train data\n",
    "docs = [\n",
    "    \"hanoi pho chaolong hanoi\",\n",
    "    \"hanoi buncha pho omai\",\n",
    "    \"pho banhgio omai\",\n",
    "    \"saigon hutiu banhbo pho\"\n",
    "]\n",
    "\n",
    "train_data, vocab = BoW().docs_to_bow_matrix(docs)\n",
    "label = np.array([\"Bắc\", \"Bắc\", \"Bắc\", \"Nam\"])\n",
    "\n",
    "print(f\"Vocabulary: {vocab}\")\n",
    "print(f\"Train_data:\\n{train_data}\\n\")\n",
    "\n",
    "# test data\n",
    "docs = [\n",
    "    \"hanoi hanoi buncha hutiu\",\n",
    "    \"pho hutiu banhbo\"\n",
    "]\n",
    "X_test, _ = BoW().docs_to_bow_matrix(docs, vocab)\n",
    "print(f\"Test_data:\\n{X_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f5ce33",
   "metadata": {},
   "source": [
    "### Sử dụng Model triển khai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "956fcdaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting class of each element: ['Bắc' 'Nam']\n",
      "Probability of each element in each class:\n",
      "[[0.89548823 0.10451177]\n",
      " [0.29175335 0.70824665]]\n"
     ]
    }
   ],
   "source": [
    "model = NBModel()\n",
    "model.fit(train_data, label)\n",
    "\n",
    "labels = model.predict(X_test)\n",
    "probas = model.predict_proba(X_test)\n",
    "\n",
    "print(f\"Predicting class of each element: {labels}\")\n",
    "print(f\"Probability of each element in each class:\\n{probas}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f995bc",
   "metadata": {},
   "source": [
    "### Sử dụng Model của Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "7d7261a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting class of each element: ['Bắc' 'Nam']\n",
      "Probability of each element in each class:\n",
      "[[0.89548823 0.10451177]\n",
      " [0.29175335 0.70824665]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(train_data, label)\n",
    "\n",
    "labels = model.predict(X_test)\n",
    "probas = model.predict_proba(X_test)\n",
    "\n",
    "print(f\"Predicting class of each element: {labels}\")\n",
    "print(f\"Probability of each element in each class:\\n{probas}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ad0bc3",
   "metadata": {},
   "source": [
    "## Bernoulli Naive Bayes\n",
    "\n",
    "Đối với bài toán bên trên, khi mã hóa dữ liệu thành một vector, ta quan tâm đến giá trị xuất hiện bao nhiêu lần trong dữ liệu đó. Đây là bài toán đơn giản hơn, khi ta quan tâm xem giá trị có xuất hiện trong bản ghi hay không?\n",
    "\n",
    "Khi đó có một công thức tổng quát đơn giản để tính $P(x_i|c)$:\n",
    "$$\n",
    "P(x_i|c) = \\lambda x_i + (1- \\lambda)(1 - x_i)\n",
    "$$\n",
    "Trong đó:\n",
    "- $\\lambda = P(x_i = 1 | c) $ tức là vị trí $i$ trong vector mã hóa có sự xuất hiện của giá trị\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
